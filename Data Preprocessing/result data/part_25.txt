uning first we'll select the model then

we'll prepare the data and then uh we'll

do the cost analysis so uh here guys I

will be doing the cost analysis and on

top of it I'll be performing the fine

tuning because the cost of the fine

tuning is very very high and it takes

lots of time as well so here definitely

I'll be guiding you related to this one

and finally we'll be starting with the

Azure open a sorry AZ your AI studio so

let's start with the fine tuning so

before starting with the fine tuning

let's try to understand the fundamental

concept of the fine tuning so here guys

uh let me show you my Blackboard and

over my Blackboard what I kept I kept

the different different model and

related to those model you find out the

parameter so uh let's look into the

parameter so this GPD 4 uh it is having

around

1.70 1.76 trillion parameter now this

GPD 3.5 is having 175 billion parameter

GPD 3 it's having again 175 billion

parameter now here you can see some

other model as well so this Bard is

having a 345 million parameter this a

t511 billion parameter okay uh let's

look into the Jimmy model So jimy Pro

it's having around 175 billion parameter

now this llama right so we have a

different different variants of the

Llama like uh 2B sorry 7B 13B 17b so it

is representing to the parameter only

now first of all we need to understand

the meaning of this parameter so let me

write over here this a parameter is

nothing guys this parameter is uh called

bits and biases okay so bits and biases

itself is called called what it's called

the parameter now whenever we are

talking about the model guys so here let

me write the model or let me write any

sort of a model let's say here I'm

writing this GPT or maybe jimy so this

model actually they are using

Transformer as a base model I think you

know about the Transformer right this

attention all you need research paper uh

you can check out with that research

paper you will get architecture of the

Transformer there you will find out the

uh inside the Transformer architecture

you'll find out the neural network there

are various stages of the Transformer so

first is uh embedding okay encoding Ming

then uh there is a multi detention and

then from the multi attention it is

going through the neural network itself

and then uh the normalization layer and

all everything so uh inside the

Transformer actually you will find out

the neural network and this neural

network actually uh it's a collection of

what weights and biases so this weights

and biases is nothing it's a trainable

parameter this is what this is trainable

parameter now just think over here if we

are saying three uh so if we are saying

like 175 billion parameter so what would

be the size of the architecture okay now

if we going to if we going to train

these type of architecture so how much

time it will take it's a like a very uh

typical question okay so like no one can

answer uh other than that person who has

trained the actual model so here we are

talking about the llm model which is a

very very huge which I just shown you so

it is having a lots of parameter like

bits and bies it is called the trainable

parameter and this bits and bies is

nothing it's a part of the neural

network only whenever we design the

neural network there actually we connect

this input layer to hi layer with a bait

and vises and Hider layer to Hidden

layer with a bait and biases okay so I

hope this thing is clear to all of you

now see guys let say we have train our

model on top of one specific data uh

what I did uh let's say this is what

this is my GPD model and this particular

GPD model I train on various data so

here this is the entertainment data now

this is the sport data okay now this is

the politic data now this is the

historical data of various data from

where from the internet and now it is

capable to answer related to this

particular data related to each and

everything now tomorrow guys let's say

there are one more data which came so

here uh one more data is related to what

uh related

to

Indian

election 24 okay so here guys see let's

say tomorrow uh we came with a we came

up with a new data and I want to ask

something okay I want to ask something

as of now see my model is just is just

train on this e-commerce data or uh

basically this